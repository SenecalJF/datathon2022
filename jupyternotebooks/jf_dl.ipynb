{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "import keras.optimizers as optimizers\n",
    "import keras.wrappers.scikit_learn as kwrap\n",
    "import keras.callbacks as callbacks\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "X_train_path = '../data/train/X_train.csv'\n",
    "y_train_path = '../data/train/y_train.csv'\n",
    "X_test_path = '../data/test/X_test.csv'\n",
    "y_test_path = '../data/test/y_test.csv'\n",
    "\n",
    "X_train_df = pd.read_csv(X_train_path)\n",
    "y_train_df = pd.read_csv(y_train_path)\n",
    "X_test_df = pd.read_csv(X_test_path)\n",
    "y_test_df = pd.read_csv(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train/version1/dataset.csv'\n",
    "test_path = '../data/test/version1/dataset.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>w.bcpi_BCPI_WEEKLY</th>\n",
       "      <th>w.bcne_BCPI_WEEKLY</th>\n",
       "      <th>w.ener_BCPI_WEEKLY</th>\n",
       "      <th>w.mtls_BCPI_WEEKLY</th>\n",
       "      <th>w.fopr_BCPI_WEEKLY</th>\n",
       "      <th>w.agri_BCPI_WEEKLY</th>\n",
       "      <th>w.fish_BCPI_WEEKLY</th>\n",
       "      <th>v39078_CA.-interest_rate</th>\n",
       "      <th>v121820_U.S.-interest_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>fxusdcad_FXUSDCAD_rsi</th>\n",
       "      <th>rsi_FXUSDCAD_rsi</th>\n",
       "      <th>pmi_ISM_MAN_PMI</th>\n",
       "      <th>index_ISM_NONMAN_NMI</th>\n",
       "      <th>1 mo_USTREASURY_YIELD</th>\n",
       "      <th>fomc_statement_sentiment_last_week</th>\n",
       "      <th>canada_bank_sentiment_last_week</th>\n",
       "      <th>fomc_presconf_sentiment_last_week</th>\n",
       "      <th>fomc_minutes_sentiment_last_week</th>\n",
       "      <th>fomc_testimony_sentiment_last_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>393.770000</td>\n",
       "      <td>303.210000</td>\n",
       "      <td>951.910000</td>\n",
       "      <td>488.660000</td>\n",
       "      <td>356.420000</td>\n",
       "      <td>207.020000</td>\n",
       "      <td>1293.840000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.331500</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.014098</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.007442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>393.902857</td>\n",
       "      <td>303.647143</td>\n",
       "      <td>951.261429</td>\n",
       "      <td>489.705714</td>\n",
       "      <td>356.127143</td>\n",
       "      <td>207.402857</td>\n",
       "      <td>1298.995714</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.324400</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.028195</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.014884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>394.035714</td>\n",
       "      <td>304.084286</td>\n",
       "      <td>950.612857</td>\n",
       "      <td>490.751429</td>\n",
       "      <td>355.834286</td>\n",
       "      <td>207.785714</td>\n",
       "      <td>1304.151429</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.321400</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.042293</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.022326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>394.168571</td>\n",
       "      <td>304.521429</td>\n",
       "      <td>949.964286</td>\n",
       "      <td>491.797143</td>\n",
       "      <td>355.541429</td>\n",
       "      <td>208.168571</td>\n",
       "      <td>1309.307143</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322267</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.056391</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.029767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>394.301429</td>\n",
       "      <td>304.958571</td>\n",
       "      <td>949.315714</td>\n",
       "      <td>492.842857</td>\n",
       "      <td>355.248571</td>\n",
       "      <td>208.551429</td>\n",
       "      <td>1314.462857</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323133</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.070489</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.037209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>394.434286</td>\n",
       "      <td>305.395714</td>\n",
       "      <td>948.667143</td>\n",
       "      <td>493.888571</td>\n",
       "      <td>354.955714</td>\n",
       "      <td>208.934286</td>\n",
       "      <td>1319.618571</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.324000</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.084586</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>0.044651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>394.567143</td>\n",
       "      <td>305.832857</td>\n",
       "      <td>948.018571</td>\n",
       "      <td>494.934286</td>\n",
       "      <td>354.662857</td>\n",
       "      <td>209.317143</td>\n",
       "      <td>1324.774286</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.321300</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.098684</td>\n",
       "      <td>0.008560</td>\n",
       "      <td>0.014384</td>\n",
       "      <td>0.052093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>394.700000</td>\n",
       "      <td>306.270000</td>\n",
       "      <td>947.370000</td>\n",
       "      <td>495.980000</td>\n",
       "      <td>354.370000</td>\n",
       "      <td>209.700000</td>\n",
       "      <td>1329.930000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.112782</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>0.059535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>395.265714</td>\n",
       "      <td>307.011429</td>\n",
       "      <td>947.797143</td>\n",
       "      <td>497.230000</td>\n",
       "      <td>355.254286</td>\n",
       "      <td>210.222857</td>\n",
       "      <td>1329.930000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.310600</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.126880</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>0.018493</td>\n",
       "      <td>0.066977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>395.831429</td>\n",
       "      <td>307.752857</td>\n",
       "      <td>948.224286</td>\n",
       "      <td>498.480000</td>\n",
       "      <td>356.138571</td>\n",
       "      <td>210.745714</td>\n",
       "      <td>1329.930000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.314100</td>\n",
       "      <td>41.796141</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.140977</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.074419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  w.bcpi_BCPI_WEEKLY  w.bcne_BCPI_WEEKLY  w.ener_BCPI_WEEKLY  \\\n",
       "0  2017-01-04          393.770000          303.210000          951.910000   \n",
       "1  2017-01-05          393.902857          303.647143          951.261429   \n",
       "2  2017-01-06          394.035714          304.084286          950.612857   \n",
       "3  2017-01-07          394.168571          304.521429          949.964286   \n",
       "4  2017-01-08          394.301429          304.958571          949.315714   \n",
       "5  2017-01-09          394.434286          305.395714          948.667143   \n",
       "6  2017-01-10          394.567143          305.832857          948.018571   \n",
       "7  2017-01-11          394.700000          306.270000          947.370000   \n",
       "8  2017-01-12          395.265714          307.011429          947.797143   \n",
       "9  2017-01-13          395.831429          307.752857          948.224286   \n",
       "\n",
       "   w.mtls_BCPI_WEEKLY  w.fopr_BCPI_WEEKLY  w.agri_BCPI_WEEKLY  \\\n",
       "0          488.660000          356.420000          207.020000   \n",
       "1          489.705714          356.127143          207.402857   \n",
       "2          490.751429          355.834286          207.785714   \n",
       "3          491.797143          355.541429          208.168571   \n",
       "4          492.842857          355.248571          208.551429   \n",
       "5          493.888571          354.955714          208.934286   \n",
       "6          494.934286          354.662857          209.317143   \n",
       "7          495.980000          354.370000          209.700000   \n",
       "8          497.230000          355.254286          210.222857   \n",
       "9          498.480000          356.138571          210.745714   \n",
       "\n",
       "   w.fish_BCPI_WEEKLY  v39078_CA.-interest_rate  v121820_U.S.-interest_rate  \\\n",
       "0         1293.840000                      0.75                        3.75   \n",
       "1         1298.995714                      0.75                        3.75   \n",
       "2         1304.151429                      0.75                        3.75   \n",
       "3         1309.307143                      0.75                        3.75   \n",
       "4         1314.462857                      0.75                        3.75   \n",
       "5         1319.618571                      0.75                        3.75   \n",
       "6         1324.774286                      0.75                        3.75   \n",
       "7         1329.930000                      0.75                        3.75   \n",
       "8         1329.930000                      0.75                        3.75   \n",
       "9         1329.930000                      0.75                        3.75   \n",
       "\n",
       "   ...  fxusdcad_FXUSDCAD_rsi  rsi_FXUSDCAD_rsi  pmi_ISM_MAN_PMI  \\\n",
       "0  ...               1.331500         41.796141             57.6   \n",
       "1  ...               1.324400         41.796141             57.6   \n",
       "2  ...               1.321400         41.796141             57.6   \n",
       "3  ...               1.322267         41.796141             57.6   \n",
       "4  ...               1.323133         41.796141             57.6   \n",
       "5  ...               1.324000         41.796141             57.6   \n",
       "6  ...               1.321300         41.796141             57.6   \n",
       "7  ...               1.325000         41.796141             57.6   \n",
       "8  ...               1.310600         41.796141             57.6   \n",
       "9  ...               1.314100         41.796141             57.6   \n",
       "\n",
       "   index_ISM_NONMAN_NMI  1 mo_USTREASURY_YIELD  \\\n",
       "0                  57.4                   0.49   \n",
       "1                  57.4                   0.51   \n",
       "2                  57.4                   0.50   \n",
       "3                  57.4                   0.50   \n",
       "4                  57.4                   0.50   \n",
       "5                  57.4                   0.50   \n",
       "6                  57.4                   0.51   \n",
       "7                  57.4                   0.51   \n",
       "8                  57.4                   0.52   \n",
       "9                  57.4                   0.52   \n",
       "\n",
       "   fomc_statement_sentiment_last_week  canada_bank_sentiment_last_week  \\\n",
       "0                            0.011111                         0.014098   \n",
       "1                            0.022222                         0.028195   \n",
       "2                            0.033333                         0.042293   \n",
       "3                            0.044444                         0.056391   \n",
       "4                            0.055556                         0.070489   \n",
       "5                            0.066667                         0.084586   \n",
       "6                            0.077778                         0.098684   \n",
       "7                            0.088889                         0.112782   \n",
       "8                            0.100000                         0.126880   \n",
       "9                            0.111111                         0.140977   \n",
       "\n",
       "   fomc_presconf_sentiment_last_week  fomc_minutes_sentiment_last_week  \\\n",
       "0                           0.001223                          0.002055   \n",
       "1                           0.002446                          0.004110   \n",
       "2                           0.003669                          0.006164   \n",
       "3                           0.004892                          0.008219   \n",
       "4                           0.006115                          0.010274   \n",
       "5                           0.007338                          0.012329   \n",
       "6                           0.008560                          0.014384   \n",
       "7                           0.009783                          0.016438   \n",
       "8                           0.011006                          0.018493   \n",
       "9                           0.012229                          0.020548   \n",
       "\n",
       "   fomc_testimony_sentiment_last_week  \n",
       "0                            0.007442  \n",
       "1                            0.014884  \n",
       "2                            0.022326  \n",
       "3                            0.029767  \n",
       "4                            0.037209  \n",
       "5                            0.044651  \n",
       "6                            0.052093  \n",
       "7                            0.059535  \n",
       "8                            0.066977  \n",
       "9                            0.074419  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'w.bcpi_BCPI_WEEKLY', 'w.bcne_BCPI_WEEKLY',\n",
       "       'w.ener_BCPI_WEEKLY', 'w.mtls_BCPI_WEEKLY', 'w.fopr_BCPI_WEEKLY',\n",
       "       'w.agri_BCPI_WEEKLY', 'w.fish_BCPI_WEEKLY', 'v39078_CA.-interest_rate',\n",
       "       'v121820_U.S.-interest_rate', 'value_FRED_DFEDTARU', 'value_FRED_DFF',\n",
       "       'value_FRED_GDPC1', 'value_FRED_GDPPOT', 'value_FRED_HSN1F',\n",
       "       'value_FRED_PAYEMS', 'value_FRED_PCEPILFE', 'value_FRED_RRSFS',\n",
       "       'value_FRED_UNRATE', 'fxusdcad_FXUSDCAD_rsi', 'rsi_FXUSDCAD_rsi',\n",
       "       'pmi_ISM_MAN_PMI', 'index_ISM_NONMAN_NMI', '1 mo_USTREASURY_YIELD',\n",
       "       'fomc_statement_sentiment_last_week', 'canada_bank_sentiment_last_week',\n",
       "       'fomc_presconf_sentiment_last_week', 'fomc_minutes_sentiment_last_week',\n",
       "       'fomc_testimony_sentiment_last_week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_to_drop = ['date', 'fxusdcad_FXUSDCAD_rsi', 'w.bcpi_BCPI_WEEKLY', 'w.bcne_BCPI_WEEKLY', 'w.ener_BCPI_WEEKLY', 'w.mtls_BCPI_WEEKLY', 'w.fopr_BCPI_WEEKLY', 'w.agri_BCPI_WEEKLY', 'w.fish_BCPI_WEEKLY', 'value_FRED_PAYEMS', 'value_FRED_DFEDTARU', 'value_FRED_DFF', 'value_FRED_GDPC1', 'value_FRED_GDPPOT', 'value_FRED_HSN1F', 'value_FRED_PCEPILFE', 'value_FRED_RRSFS',]\n",
    "X_train,y_train = train.drop(elem_to_drop, axis=1),train['fxusdcad_FXUSDCAD_rsi'].values\n",
    "X_test, y_test = test.drop(elem_to_drop, axis=1),test['fxusdcad_FXUSDCAD_rsi'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = np.asarray(X_train), np.asarray(X_test), np.asarray(y_train), np.asarray(y_test)\n",
    "# y_train = np.asarray(y_train_df)\n",
    "\n",
    "# test = np.asarray(X_test_df)\n",
    "# y_test = np.asarray(y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-2, input_shape=[23]):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    optimizer = optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "model = build_model(n_hidden=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_35\" is incompatible with the layer: expected shape=(None, 23), found shape=(None, 17)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [191], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_fit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei5bqjzhc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jeanj\\Programming\\Maching learning\\Datathon-2022-data\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_35\" is incompatible with the layer: expected shape=(None, 23), found shape=(None, 17)\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(X_train, y_train, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1943,)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to 0-1\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: -0.13\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "svm = svm.SVR()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(svm.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3261033259326944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression(n_jobs=-1).fit(X_train, y_train)\n",
    "y_pred_linearReg = reg.predict(X_test)\n",
    "reg.score(X_test, y_test)\n",
    "print(reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n",
    "xg_reg = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.06,\n",
    "                max_depth = 10, alpha = 10, n_estimators = 200)\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "polynomial_svm_reg = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=4)),\n",
    "        (\"svm_clf\", SVR(kernel=\"rbf\", C=5, coef0=1, gamma=5))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_reg.fit(X_train, y_train)\n",
    "y_pred_poly = polynomial_svm_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "polynomiale_sgd_reg = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2)),\n",
    "    (\"sgd_reg\", SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.1))\n",
    "    ])\n",
    "# sgdReg = SGDRegressor(max_iter=10000, tol=1e-3, eta0=0.1, loss=\"epsilon_insensitive\")\n",
    "polynomiale_sgd_reg.fit(X_train, y_train)\n",
    "y_pred_sgd = polynomiale_sgd_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001259619523929103 xgboost\n",
      "-0.13639657098304236 xgboost\n",
      "0.0012493269137149822 svm\n",
      "-0.12711084086248392 svm\n",
      "0.002578329817984787 linear regression\n",
      "-1.3261033259326944 linear regression\n",
      "0.0014215329976240507 polynomial regression\n",
      "-0.2824707726030291 polynomial regression\n",
      "0.011069720114690403 sgd regression\n",
      "-8.986818829893128 sgd regression\n"
     ]
    }
   ],
   "source": [
    "#calculate r2 score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "print(mse, 'xgboost')\n",
    "print(r2, 'xgboost')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(mse, 'svm')\n",
    "print(r2, 'svm')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_linearReg)\n",
    "r2 = r2_score(y_test, y_pred_linearReg)\n",
    "print(mse, 'linear regression')\n",
    "print(r2, 'linear regression')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_poly)\n",
    "r2 = r2_score(y_test, y_pred_poly)\n",
    "print(mse, 'polynomial regression')\n",
    "print(r2, 'polynomial regression')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_sgd)\n",
    "r2 = r2_score(y_test, y_pred_sgd)\n",
    "print(mse, 'sgd regression')\n",
    "print(r2, 'sgd regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1943, 12)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 12)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_30 (LSTM)              (None, 1, 32)             5760      \n",
      "                                                                 \n",
      " lstm_31 (LSTM)              (None, 1, 16)             3136      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1, 1)              17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,913\n",
      "Trainable params: 8,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "55/55 [==============================] - 4s 16ms/step - loss: 1.1072 - accuracy: 0.0000e+00 - val_loss: 0.3265 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.0000e+00 - val_loss: 0.0235 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.0000e+00 - val_loss: 0.0066 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.0000e+00 - val_loss: 0.0033 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 0.0000e+00 - val_loss: 0.0029 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - val_loss: 0.0032 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - val_loss: 0.0030 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0052 - accuracy: 0.0000e+00 - val_loss: 0.0046 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0043 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 0.0000e+00 - val_loss: 0.0033 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00 - val_loss: 0.0028 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 0.0000e+00 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 0.0000e+00 - val_loss: 0.0027 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00 - val_loss: 0.0023 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 0.0000e+00 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 9.9184e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 8.1708e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 7.9103e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 0.0000e+00 - val_loss: 9.3611e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 8.1643e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 6.1639e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 5.8357e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 5.2692e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 6.7392e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 6.6262e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.9058e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - val_loss: 6.9411e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.3557e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.2399e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 5.0786e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.0447e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.9567e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 6.1687e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.6734e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.0284e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.6747e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.6363e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.6570e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.0677e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.9415e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.6337e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.3337e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.8457e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.5649e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 4.5487e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.5538e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.0094e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 4.1153e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.8601e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 4.2704e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.9162e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 5.8363e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 3.8796e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 9.6974e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 5.0066e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 4.8259e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 8.6545e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 6.7150e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 4.3639e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 2.6903e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.8652e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 9.7442e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 3.4213e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 5.0211e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 6.1219e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 6.5697e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 3.8410e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 9.0011e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 4.7273e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 4.3100e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 6.9956e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 5.4421e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 5.3338e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 7.3227e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 7.5189e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 3.7366e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 8.3222e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 8.0593e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 9.2307e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 7.5443e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.0000e+00 - val_loss: 9.5094e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 9.6909e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 7.0935e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00 - val_loss: 5.8824e-04 - val_accuracy: 0.0000e+00\n",
      "5/5 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "#reshape data\n",
    "X_train, X_test = np.asarray(X_train), np.asarray(X_test)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "model.add(LSTM(32, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "y_pred_lstm = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001261118418778472 y_pred_lstm_mse\n",
      "-0.13774883564290863 y_pred_lstm_r2\n"
     ]
    }
   ],
   "source": [
    "y_pred_lstm = y_pred_lstm.reshape(-1)\n",
    "print(mean_squared_error(y_test, y_pred_lstm), \"y_pred_lstm_mse\")\n",
    "print(r2_score(y_test, y_pred_lstm), \"y_pred_lstm_r2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd495b64596ba7dd17d77d1b04e38d9236653d58e5b7bd568f787c46e245586c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
